# Plan 10: Cloudflare-Hosted Solid Pod Server

## Investigation Status: Research Complete

This document records findings from investigating Cloudflare as a hosting platform for a W3C Solid-compatible pod server. Three architectural options are described, with a practical recommendation.

---

## Background: What a Solid Pod Server Must Do

A spec-compliant Solid server implements:

1. **LDP (Linked Data Platform)** — GET/PUT/POST/DELETE/PATCH/HEAD on RDF and binary resources, container listings, Link headers indicating LDP types.
2. **WAC (Web Access Control)** — `.acl` documents stored as regular resources; enforced on every request.
3. **Solid OIDC** — An OIDC provider that issues ID tokens with a `webid` claim; access tokens are DPoP-bound.
4. **DPoP verification** — Every authenticated request carries a DPoP proof JWT that the server verifies.
5. **(Optional) WebSocket PubSub** — Solid Notifications protocol for live resource change events.

The key constraint: every request may need to read/write resource data, enforce access control, and verify cryptographic proofs — all within the request/response cycle.

---

## Cloudflare Services: Fit for Solid

### Workers (Compute)

Workers runs TypeScript in V8 isolates at ~300 edge PoPs. It is **not** Node.js; the `nodejs_compat` flag provides polyfills for many Node.js APIs but not a real filesystem.

| Constraint | Free | Paid |
|---|---|---|
| CPU time/request | 10 ms | 30 s (up to 5 min) |
| Memory/isolate | 128 MB | 128 MB |
| Request body | 100 MB | 500 MB (Enterprise) |
| Worker script (compressed) | 3 MB | 10 MB |
| Persistent local filesystem | None | None (virtual FS is transient) |

**For LDP routing and DPoP verification**: Workers are well-suited. The 30-second CPU limit on paid tier is far more than needed for LDP operations.

**For running CSS or NSS**: Not viable. CSS ([Community Solid Server](https://github.com/CommunitySolidServer/CommunitySolidServer)) and NSS ([Node Solid Server](https://github.com/nodeSolidServer/node-solid-server)) are the two main open-source Solid server implementations — both are Node.js applications. Both depend on `node:fs` for persistent storage, have large dependency trees exceeding the 10 MB bundle limit, and rely on startup-time initialization (Components.js for CSS) incompatible with the isolate model.

### R2 (Object Storage)

R2 implements the core S3 API and is the natural fit for Solid resource bodies.

Key properties relevant to Solid:
- **ETags**: Supported. Conditional requests (`If-Match`, `If-None-Match`, `If-Modified-Since`) work on both GET and PUT — essential for LDP concurrency control.
- **Conditional PUT**: `PutObject` with `If-Match`/`If-None-Match` enables optimistic locking, preventing lost updates on concurrent writes to the same resource.
- **Strong per-object consistency**: Read-after-write consistency, unlike S3's historical eventual consistency on some paths.
- **Custom metadata**: Arbitrary `x-amz-meta-*` headers stored per object — useful for content-type, LDP type hints, and ACL document pointers.
- **Free egress**: No data transfer costs, unlike most cloud storage providers.

LDP-to-R2 mapping:

| LDP Concept | R2 Representation |
|---|---|
| `ldp:Resource` | R2 object at the resource's path as key |
| `ldp:Container` | Key prefix (e.g. `alice/photos/`); no real directory object |
| Container listing (Turtle) | Generated at request time by listing objects with prefix |
| `.acl` documents | Regular R2 objects at `resource.ttl.acl` keys |
| ETags / conditional writes | R2 native ETags + conditional PUT headers |
| Content-Type | R2 `httpMetadata.contentType` |
| LDP Link headers | Added by the Worker at response time |

The main gap: R2 has no true directory concept, so container Turtle documents must be generated by the Worker by listing objects under a prefix. This works well for pods of typical size; very large containers (10k+ resources) will need pagination.

**Pricing (free tier is generous for personal pods):**

| Metric | Free | Paid |
|---|---|---|
| Storage | 10 GB-month | $0.015/GB-month |
| Class A ops (PUT, LIST) | 1M/month | $4.50/million |
| Class B ops (GET, HEAD) | 10M/month | $0.36/million |
| Egress | Free | Free |

### D1 (Edge SQLite)

D1 is SQLite backed by Durable Objects internally, with automatic read replicas.

Good uses for Solid metadata:
- Resource ETag and content-type index (avoids `HeadObject` calls on hot paths)
- Container membership index (faster than R2 prefix listing for large containers)
- OIDC: client registrations, session records, token metadata
- WAC policy cache (parsed ACL rules, for fast authorization without reading R2 `.acl` files)

**Critical limitation — write latency**: D1 has a single primary (chosen at database creation). Writes from users far from that primary (e.g. Tokyo writing to a US-East primary) will see 150–200 ms of added latency. Cloudflare's Smart Placement can co-locate the Worker with the D1 primary, but then all global users route to one region. For write-heavy workloads (every LDP PUT updates metadata), this is a real trade-off.

D1 is best used for **read-heavy metadata lookups** that benefit from read replicas, with Durable Objects handling writes that need strong global consistency.

**Free tier limits:**

| Metric | Free | Paid |
|---|---|---|
| Row reads/day | 5M | 25B/month included |
| Row writes/day | 100K | 50M/month included |
| Max DB size | 500 MB | 10 GB |

### KV (Key-Value)

Workers KV is eventually consistent: writes propagate globally in up to 60 seconds. This makes it **unsuitable as the primary metadata store** for a Solid server — a revoked ACL could still grant access to users in other regions for up to a minute after the revocation.

KV is acceptable for caching stable data: JWKS public keys from the OIDC provider, long-lived public WebID profiles, or other data where stale reads are tolerable.

### Durable Objects

Durable Objects provide a single-threaded, strongly consistent execution unit with persistent state (including SQLite storage added in 2024, up to 10 GB per DO instance). Each DO lives at exactly one Cloudflare data center.

For a Solid pod service, a natural mapping is **one Durable Object per pod**:
- Strong consistency for that pod's write operations (ETag updates, ACL changes, container structure)
- Per-pod write serialization (prevents concurrent-write conflicts without distributed locking)
- SQLite storage within the DO for pod metadata, complementing R2 for resource bodies
- WebSocket hibernation support for Solid Notifications PubSub

**Pricing:**

| Metric | Free | Paid |
|---|---|---|
| Requests/month | 1M | $0.15/million |
| Wall-clock duration | 400K GB-s | $12.50/million GB-s |
| Storage (SQLite) | 1 GB | $0.20/GB-month |

---

## DPoP and OIDC on Workers

### DPoP Verification

DPoP requires verifying an ECDSA (P-256) or EdDSA signed JWT on every authenticated request. Workers expose the full WebCrypto API (`crypto.subtle`), and the **`jose` library** (the reference JOSE implementation for JavaScript) explicitly lists Cloudflare Workers as a supported runtime.

DPoP proof verification — including `htu`/`htm` claim matching, `jti` replay prevention, and `ath` (access token hash) checking — is fully achievable in Workers with no known limitations. ECDSA P-256 verification is sub-millisecond in WebCrypto.

### Solid OIDC Provider

A complete Solid OIDC provider must:
1. Issue ID tokens with a `webid` claim
2. Issue DPoP-bound access tokens
3. Expose `/.well-known/openid-configuration` and JWKS endpoint
4. Handle authorization code + PKCE flow
5. Manage client registrations, authorization codes, refresh tokens, and sessions

Cloudflare's **`workers-oauth-provider`** library (official, TypeScript, MIT) implements OAuth 2.1 with PKCE on Workers. It uses Durable Objects for session state. Extending it for Solid OIDC requires:
- Adding the `webid` claim to ID tokens
- Enforcing DPoP binding at token issuance
- WebID-OIDC discovery (linking the OpenID Provider to the WebID document)

This is achievable as a targeted implementation effort. Existing Workers OIDC projects (`eidam/cf-access-workers-oidc`, `IDPFlare`) provide reference patterns.

Alternatively, the OIDC provider can be a **separate service**: CSS running on a VPS provides a complete OIDC provider, or a hosted Solid IdP (e.g., solidcommunity.net) can be used for identity while a custom Workers layer handles the LDP pod.

---

## User Identity / Pod Ownership

In Solid, identity is a **WebID** — a URL that resolves to an RDF profile document (Turtle) containing the user's public key material, OIDC provider link, and other claims.

For a Cloudflare-hosted pod:
- The pod owner's WebID would be a URL on the pod domain (e.g. `https://pod.example.com/alice/profile/card#me`)
- The WebID document is stored as an R2 object (a Turtle file)
- The document links to the OIDC provider via `solid:oidcIssuer` (pointing to the Workers OIDC provider URL or a third-party IdP)
- On first access, the pod bootstraps by creating the WebID profile document and the root container ACL

For multi-user pod hosting, each user gets a pod at `https://pod.example.com/{username}/` with their own WebID and ACL documents.

---

## Sharing Data (Read-Only)

Solid uses **Web Access Control (WAC)** for authorization. A `.acl` document at `resource.acl` (or inherited from a parent container's `.acl`) specifies who can read/write/control a resource.

To grant read-only access to a specific user:
```turtle
# /alice/notes/draft.ttl.acl
@prefix acl: <http://www.w3.org/ns/auth/acl#> .

<#owner>
  a acl:Authorization ;
  acl:agent <https://pod.example.com/alice/profile/card#me> ;
  acl:accessTo <./draft.ttl> ;
  acl:mode acl:Read, acl:Write, acl:Control .

<#bob-read>
  a acl:Authorization ;
  acl:agent <https://pod.otherdomain.com/bob/profile/card#me> ;
  acl:accessTo <./draft.ttl> ;
  acl:mode acl:Read .
```

To grant public (unauthenticated) read access, use `acl:agentClass foaf:Agent`.

The Worker enforces WAC on every request:
1. Read the resource's `.acl` file (or walk up to the nearest parent `.acl`) from R2
2. Parse the Turtle ACL document in memory
3. Match the requesting agent's WebID (from the verified DPoP-bound token) against the authorization rules
4. Allow or deny the request

For unauthenticated public reads (common for Solid profiles and public resources), no token verification is needed — just check that `acl:agentClass foaf:Agent` grants `acl:Read` mode.

---

## Architectural Options

### Option A: VPS + Cloudflare (Recommended for Today)

**Best for**: A working, spec-compliant Solid Pod server with minimal custom code.

```
User ──→ Cloudflare (DNS, TLS, WAF, CDN cache for public resources)
              │
              ▼
         Cloudflare Tunnel (cloudflared)
              │
              ▼
         VPS (Fly.io / Hetzner / Render)
              ├── Community Solid Server (Docker)
              └── Persistent Volume (pod data)
```

CSS is the most complete open-source Solid server. It implements LDP, WAC, Solid OIDC, and WebSocket notifications out of the box. Deploying on Fly.io with a persistent volume and a Cloudflare Tunnel gives you:
- Working Solid spec compliance on day one (no LDP implementation from scratch)
- DDoS protection and WAF from Cloudflare
- Free TLS certificate management
- CDN caching for public pod resources
- No static IP required (Tunnel handles outbound-only connectivity)

**Cost**: ~$4–10/month for a small Fly.io machine + volume. Cloudflare Free plan covers DNS, TLS, and basic WAF.

**Limitations**:
- Single region (pick the closest to your primary user base)
- Persistent disk requires a backup strategy
- Cloudflare R2/D1 not used for pod data (CSS has no R2 storage backend as of early 2026)
- Scaling requires manual work (CSS is not horizontally scalable with filesystem storage)

### Option B: Native Workers + R2 + Durable Objects (Greenfield Build)

**Best for**: A globally distributed, multi-user Solid pod service with no single-region bottleneck.

```
User ──→ Cloudflare Workers (LDP HTTP layer, DPoP verification, WAC enforcement)
              ├── R2 (resource bodies: Turtle files, binary assets, .acl files)
              ├── Durable Objects (per-pod: ETag authority, write serialization, container index)
              ├── D1 (global metadata: OIDC client registrations, token records)
              └── Workers (OIDC provider, separate Worker using workers-oauth-provider)
```

Each pod maps to one Durable Object instance. The DO holds the pod's authoritative metadata (ETags, container structure, ACL document locations) and serializes writes. Resource bodies live in R2. The OIDC provider is a separate Worker deployment.

**What needs to be built from scratch**:
- LDP HTTP layer (Hono on Workers is a good starting point)
- WAC enforcement engine (Turtle parser + ACL evaluation in Workers memory)
- N3 Patch handler for SPARQL Update on RDF resources
- Container listing generator (R2 prefix list → Turtle representation)
- Solid OIDC provider (extend `workers-oauth-provider` with WebID + DPoP claims)
- WebID bootstrap flow (first-time pod setup, profile document creation)
- Solid Notifications (WebSocket via DO hibernation)

**Key implementation risks**:
- An RDF/Turtle parser (`n3.js`) in Workers: needs memory profiling against the 128 MB limit for large graphs
- Container listings for very large containers (R2 prefix listing at scale)
- DO cold-start latency on first access to a pod instance (typically < 100 ms, but affects p99)

**No existing open-source implementation** of a native Cloudflare Workers Solid server exists as of early 2026. This is a substantial engineering project.

### Option C: Hybrid (Incremental Path)

```
User ──→ Cloudflare Workers (edge: DPoP verify, public resource cache, routing)
              │                          │
              ▼                          ▼
         CF Cache / KV            Cloudflare Tunnel ──→ VPS running CSS
         (public WebID profiles,       (LDP + OIDC for authenticated requests)
          JWKS, public resources)
```

Workers handle unauthenticated reads (public Solid profiles, public pod content) from CF Cache or KV, offloading the VPS. Authenticated requests tunnel through to CSS. This reduces VPS load for public-read-heavy workloads (common for social graph data) while keeping CSS as the source of truth.

---

## Recommendation

**Start with Option A (VPS + Cloudflare)** if the goal is a working Solid Pod server in the near term:

1. Deploy CSS on Fly.io (`solidproject/community-server` Docker image) with a persistent volume.
2. Configure Cloudflare DNS for the pod domain with proxy enabled.
3. Run `cloudflared` alongside CSS for tunnel connectivity.
4. Configure CSS for production (disable in-memory mode, set file-based storage, configure HTTPS).

**Invest in Option B (native Workers)** if the longer-term goal is a scalable, multi-user pod service hosted entirely on Cloudflare with no VPS dependency. The key technology choices are:
- Hono for the HTTP layer
- `n3.js` for Turtle/RDF parsing (needs Workers compatibility testing)
- `workers-oauth-provider` + Solid OIDC extensions for identity
- One Durable Object per pod for strong consistency
- R2 for all resource bodies

There is no off-the-shelf Solid server that runs natively on Cloudflare Workers today. Option B is a meaningful engineering undertaking.

---

## Organization-Owned Pods and Per-User Access Control

### What the Spec Supports

**Organization-owned storage** is explicitly supported at the ownership level. The Solid spec states storage is "controlled by individuals *or organizations*." An organization can have its own WebID (e.g. `https://acme.example.com/profile#org`) — a URI resolving to an RDF profile document — and own a pod under that identity. Architecturally, this is no different from an individual-owned pod.

**Group-based access via `acl:agentGroup`** is WAC's mechanism for granting access to a set of users. An authorization references a group document:

```turtle
<#team-read>
  a acl:Authorization ;
  acl:agentGroup <https://acme.example.com/groups/engineering#group> ;
  acl:accessTo <./project/> ;
  acl:mode acl:Read .
```

The group document at that URL is a `vcard:Group` listing `vcard:hasMember` WebIDs. The server fetches that document at authorization time to determine membership. This is the primary spec-defined mechanism for per-user access within an org pod.

### Gaps and Limitations

| Concern | Status |
|---|---|
| Nested / hierarchical groups | Not supported in WAC. No `admin → manager → member` role hierarchy. |
| Role-based access control (RBAC) | Not in the spec. WAC is ACL-based — access is by identity, not by role. |
| Group membership staleness | Unspecified. Servers must re-fetch the group document per request, or cache it (risking stale membership after a member is removed). |
| Group document visibility | The group membership document must be readable by the authorizing server — in practice often means publicly readable, which may be undesirable for internal org data. |
| Per-user ownership within a shared pod | No spec concept of this. Pod ownership is monolithic; ACLs handle access, not sub-ownership. |
| Audit trails / per-user activity logs | Not addressed by the spec. |
| Dynamic group invalidation | No push mechanism. Servers have no way to know a group document changed; staleness window depends on caching strategy. |

### ACP — the Successor Model

ACP (Access Control Policy) is the newer, more expressive alternative to WAC. It introduces composable "Matchers" and "Policies" that can express more sophisticated conditions, closer to RBAC. ACP is implemented by Inrupt's Enterprise Solid Server (ESS) but **not yet by CSS** (as of early 2026) and is not yet in the core Solid protocol spec — it exists as a competing proposal alongside WAC. The open question of which becomes the canonical authorization mechanism is unresolved in the Working Group.

### Practical Pattern for an Org Pod Today

An organization pod using WAC would:
1. Create a WebID for the organization itself
2. Maintain `vcard:Group` membership documents as living RDF files (managed by the org, stored in the pod or elsewhere)
3. Write per-container or per-resource `.acl` files referencing those groups
4. Accept no RBAC — access is granted by identity, not by role
5. Accept no group hierarchy — sub-teams need their own separate group documents

### Implications for This Project

For a Cloudflare-hosted Solid server serving organizational data (e.g. a shared team repository):
- **WAC is sufficient for simple access tiers** (e.g. "this team can read, this admin WebID can write")
- **WAC becomes burdensome** for organizations with many groups, nested teams, or frequently-changing membership — the group document management overhead is significant
- **ACP would be the right long-term answer** if the server targets org/enterprise use cases, but it adds implementation complexity and reduces interoperability with WAC-only clients and servers
- For Option B (native Workers implementation), the access control model choice (WAC vs ACP) is a key early architectural decision — WAC is simpler to implement and more widely supported today

---

## Spec Compliance Testing

There are established test suites that run against a live Solid server URL via authenticated HTTP — they are hosting-agnostic and can be pointed at a Cloudflare-hosted server just as easily as CSS on a VPS.

### `solid-contrib/conformance-test-harness`

[GitHub](https://github.com/solid-contrib/conformance-test-harness) — the primary harness used by the Solid community. Actively maintained (v1.2.2, November 2025).

- Loads test cases from [`solid/specification-tests`](https://github.com/solid/specification-tests), which map directly to clauses in the Solid specs
- Outputs EARL-format reports (HTML+RDFa), suitable for publishing compliance results
- Requires two user accounts on the server under test (conventionally named Alice and Bob)
- Configurable via YAML: point it at any server URL, supply credentials, run

### `solid-contrib/test-suite` / solidservers.org

[GitHub](https://github.com/solid/test-suite) / [solidservers.org](https://solidservers.org/) — a broader collection of test modules:

| Module | Coverage |
|---|---|
| CRUD tests (Jest) | GET/PUT/POST/DELETE on resources and containers |
| WAC tests | Web Access Control enforcement correctness |
| WebID provider tests | Identity/OIDC flow |
| WebSockets Pub-Sub | Solid Notifications (optional) |
| Concurrency tests | Simultaneous writes to the same resource |

[solidservers.org](https://solidservers.org/) publishes live compliance results across known implementations. As of early 2026:
- NSS, PHP Solid Server, Solid-Nextcloud: all core tests passing
- CSS: passes most tests; some WAC edge cases flagged
- Inrupt ESS: uses ACP instead of WAC, so WAC tests don't apply

### `solid-contrib/web-access-control-tests`

[GitHub](https://github.com/solid-contrib/web-access-control-tests) — focused specifically on WAC enforcement correctness. Useful for targeted testing of the authorization layer in isolation.

### Relevance to a Cloudflare-Native Server

For Option B (native Workers implementation), running the conformance test harness against the deployed Worker is the natural validation gate for each implemented feature. The suggested development sequence:
1. Pass basic CRUD tests before implementing WAC
2. Pass WAC tests before implementing OIDC/DPoP
3. Run the full conformance harness as a CI step once a stable baseline is reached

---

## Open Questions for Future Investigation

- [ ] Can `n3.js` (RDF/Turtle parser) run within the Workers 128 MB memory limit for realistic Solid resource sizes?
- [ ] What does a per-pod Durable Object's SQLite schema look like for container indexing and ETag tracking?
- [ ] Does Cloudflare's `workers-oauth-provider` library require Durable Objects, or can it use D1 only?
- [ ] Is there any community work on a CSS storage adapter for S3/R2 that could land before mid-2026?
- [ ] For the hybrid option: what is the latency overhead of routing authenticated requests through a Worker before tunneling to CSS?
- [ ] ACP (Access Control Policy, the newer alternative to WAC) — which access control model should be targeted?
- [ ] Run the `solid-contrib/conformance-test-harness` against a deployed Option A (CSS on Fly.io) instance to establish a baseline before any custom development.
